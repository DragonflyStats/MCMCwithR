% - http://www.mind.ilstu.edu/curriculum/protothinker/natural_language_processing.php

Introduction

In this paper I present a general introduction to natural language processing. This is primarily a discussion of how one might go about getting a computer to process a natural language. I also describe how PT-Thinker appears to process English.

"Natural language processing" here refers to the use and ability of systems to process sentences in a natural language such as English, rather than in a specialized artificial computer language such as C++. The systems of real interest here are digital computers of the type we think of as personal computers and mainframes (and not digital computers in the sense in which "we are all digital computers," if this is even true). Of course humans can process natural languages, but for us the question is whether digital computers can or ever will process natural languages.

Unfortunately there is some confusion in the use of terms, and we need to get straight on this before proceeding. First of all, occasionally the phrase "natural language" is used not for actual languages as they are used in ordinary discourse, such as our actual use of English to communicate in everyday life, but for a more restricted subset of such a human language, one purged of constructions and ambiguities that computers could not sort out. Hence one writer states that "human languages allow anomalies that natural languages cannot allow."2 There may be a need for such a language, but a natural language restricted in this way is artificial, not natural. I do not use the phrase "natural language" in this restricted sense of an artificial natural language. When I use the phrase, I mean human language in all its messiness and varied use.

Second, the phrase "natural language processing" is not always used in the same way. There is a broad sense and a narrow sense.1 The phrase sometimes is taken broadly to include signal processing or speech recognition, context reference issues, and discourse planning and generation, as well as syntactic and semantic analysis and processing (the meaning of these terms will be discussed more fully later). At other times the phrase is used more narrowly to include only syntactic and semantic analysis and processing.

Third, even if this confusion is overcome, the phrase "natural language processing" may or may not be taken as synonymous with "natural language understanding." "Processing" most naturally is used for both interpretation and generation, while one would think "understanding" is better used for only the interpretation part. To me, to say that a system is capable of natural language understanding does not imply that the system can generate natural language, only that it can interpret natural language. To say that the system can process natural language allows for both understanding (interpretation) and generation (production). But the phrase "natural language understanding" seems used by some authors as synonymous with "natural language processing," and on this use includes interpretation and generation. In this paper I�ll use the phrase natural language processing, but keep in mind I�m mostly just discussing interpretation rather than generation.

Fourth, another problem is that the phrase "natural language understanding" is sometimes used in the sense of research into how natural languages work and the attempt to develop a computational model of this working, with "natural language processing" referring to a system interpreting and generating natural languages. This use of the phrase alludes to two distinct goals of this field of research. One goal is to understand how natural language processing works; here "natural language understanding" is a human endeavor to understand natural language processing, whoever does the processing. Another goal is to get a computer to process natural languages, and of course in this attempt to build a natural language processor fulfilling the first goal of understanding how a processor works could be useful. Another way to distinguish these two goals is to note that in seeking to develop a computational model of natural language processing (or understanding), one must distinguish between the scientific goal and the technical goal. In this attempt to develop a computational model, there is the scientific goal or motivation of understanding natural language comprehension and production for its own sake. There is also usually associated the technical goal of getting a computer to process natural language sentences.3 Allen at first seems to distinguish natural language understanding as a human scientific endeavor from natural language processing as something that endeavor is trying to investigate and model, but then at times even he uses "natural language understanding" in a way that requires one to figure out which sense is intended.

There is a fifth confusion to sort out, and this is that the term "understanding" is loaded with implications or connotations that aren�t present with "processing," though most users of the phrase "natural language understanding" seem either unaware or unconcerned about these loaded connotations. Humans are of course able to process and understand natural languages, but the real interest in natural language processing here is in whether a computer can or will be able to do it. Because of the connotations of the term "understanding," it�s use in the context of computer processing should be qualified or explained. Searle, for example, claims that digital computers such as PCs and mainframes, as we currently know them, cannot understanding anything at all, and no future such digital computer will ever be able to understand anything by virtue of computation alone. Others disagree. In view of this controversy over whether digital computers of the type under consideration can ever understand anything, use of the phrase "natural language understanding" seems to slant the issue unless one qualifies the use of the phrase "understanding." But many authors use the phrase "understanding" as if the term had no controversial history with respect to computers. I think that Searle�s claims, whether correct or not, are significant and need to be addressed, and one shouldn�t go slinging around the term "understanding" without noting that it is not necessarily implied that computers can understand in the sense to which Searle objects. The term "processing" is perhaps preferable to "understanding" in this context, but "understanding" has a history here and I am not advocating we discontinue use of the term. Certainly in this paper if I use the terms "understanding" or "knowledge" metaphorically with reference to computers, I imply nothing about whether they can or will ever really understand or know in any philosophically interesting sense.

How much ability would count as a natural language processing capability? Not all humans can process natural language at the same level, so we cannot answer this question precisely, but the ability to interpret and converse with humans in normal, ordinary human discourse would be the goal. This would be no mean feat. "Processing" means translating from or into a natural language (interpretation or generation). To be able to converse with other humans, even if restricted to textual interaction rather than speech, a computer would probably need not only to process natural language sentences but also possess knowledge of the world. A decent conversation would involve interpretation and generation of natural language sentences, and presumably responding to comments and questions would require some common-sense knowledge. As we shall see such common-sense knowledge would be needed even to grasp the meaning of many natural language sentences. For instance, if the computer were told "Mary had a little lamb," common-sense knowledge would allow the computer to realize that Mary might have eaten lamb for dinner rather than owned a lamb, but the same type of common sense knowledge would show this interpretation is not possible for "Mary had a little washing machine." Common sense is needed to know that humans eat lamb but not washing machines.

I�m not going to discuss all the possible uses of a computer that could process a natural language, but I�ll mention some here. Typical applications for natural language processing include the following.4

A better human-computer interface that could convert from a natural language into a computer language and vice versa. A natural language system could be the interface to a database system, such as for a travel agent to use in making reservations. Blind people could use a natural language system (with speech recognition) to interact with computers, and Steven Hawking uses one to generate speech from his typed text.

A translation program that could translate from one human language to another (English to French, for example). Even if programs that translate between human languages are not perfect, they would still be useful in that they could do the rudimentary translation first, with their work checks and corrected by a human translator. This cuts down on the time for the translation.

Programs that could check for grammar and writing techniques in a word processing document.

A computer that could read a human language could read whole books to stock its database with data. This would surely be easier than what Doug Lenat is trying to do with CYC.

Obviously, probably it would be easier to get a computer to accomplish a task if you could talk to it in normal English sentences rather than having to learn a special language only a computer and other programmers can understand. I say "probably" because programming languages typically require you to state things in a way that is more precise than occurs in typical English sentences, and the average person might be hard-pressed to get the computer to do some mathematically precise task if he or she had to state the task in conversational English. But on the face of it, at least, it would seem to be a great thing if we could converse with computers as we do with one another.

What does natural language processing include?

Broadly construed, natural language processing (with respect to the interpretation side) is considered to involve at least the following subtopics5:

Signal processing
Syntactic analysis
Semantic analysis
Pragmatics
For the most part there is agreement on what fits into each of these categories. I use the term "pragmatics" here broadly; some authors use it more narrowly and add categories that I consider to be within pragmatics broadly.

Signal processing takes spoken words as input and turns it into text. Syntactic analysis gets at the structure or grammar of the sentences. Semantic analysis deals with the meaning of words and sentences, the ways that words and sentences refer to elements in the world. "Meaning" in these discussions is usually associated with semantics, but in other contexts I�ve seen syntax associated with "syntactic meaning." Pragmatics concerns how the meaning of a sentence depends on its function in everyday life, that is, the larger context of the conversation and so forth, and so it too seems concerned with meaning. So it might be best not to think of "meaning" issues as restricted to just semantics.

In this paper I�ll mostly ignore the topic of signal processing. Computers most often take text input directly, whether at the keyboard or read from a file or other source, rather than interpret spoken language. There are some sophisticated systems, and even some less costly ones anybody can buy, that process spoken words more or less successfully to translate them into text form.

I will discuss some of the basics of the other topics. James Allen has the second edition of what is considered the standard work here, Natural Language Understanding, and I draw from that source frequently.

Before proceeding I should note, however, two other processes that have to occur for natural language processing: tokenization and machine level processing. Some authors mention tokenization as another component of natural language processing, and it seems to me that you could consider it as part of signal processing, between signal processing and syntactic analysis, or perhaps less easily even as part of syntactic analysis.7 Tokenization is the conversion of an input signal into parts (tokens) so that the computer can process it. For example, even when a machine receives text from the keyboard, the text enters the machine as stream of ASCII characters, which must be classified into individual units, such as words or letter/number characters.6 The computer doesn�t know how to consider the ASCII characters types into it unless it is told to expect characters, integers, floating point numbers, a string, etc. These are data types. When you type a letter on the keyboard, for example, the effect is to transmit an ASCII character code for the particular letter typed. "A," for example, is 41 (this would be transmitted in binary). But the computer must take this ASCII character and join it to others to interpret them together as a word, or an integer, or some other type of unit; in short, a token. Insofar as some authors talk of signal processing as the conversion of speech into text, and the text is recognized as words, some tokenization might already be involved. However, one might instead think of the signal processing as purely a conversion of audio to some kind of textual stream, with no processing of the stream as words or individual characters per se, i.e., no data typing on the part of the computer. In this case, tokenization would seem to be an additional component of natural language processing and not just part of signal processing. In this way some authors write of tokenization as the first step of parsing and preceding syntactic analysis. In this sense tokenization is needed not just for natural language processing but for any language processing on the part of the computer. Or if one has a really broad notion of syntactic analysis, one might even see this as including the tokenization, since recognizing words as opposed to numbers already involves the grammar of a language.

Another topic that is usually left out of the picture in discussing natural language processing in computers is machine level processing: the fact that, as the signal is processed, and syntactic, semantic, and pragmatics analysis are being accomplished, there is the continual translation into machine language, or even further, voltage signals. This will occur for the natural language sentence that is being analyzed, but also for all of the processing commands that the computer is following as it analyzes. How this occurs will involve the particular programming language in which the natural language processing program is written. But this code generation occurs as the program is compiled/interpreted and run.

In discussions of natural language processing by computers, it is just presupposed that machine level processing is going on as the language processing occurs, and it is not considered as a topic in natural language processing per se. But I think that if the attempt is being made to understand how natural language processing occurs in humans, with the goal of using this understanding to build a digital computer system that accomplishes satisfactory natural language processing, then the topic of how the actual low-level processing actually occurs in humans and digital computers may not be irrelevant. If humans process natural languages by using parallel, distributed processing at the basic level, and this is not just an implementation of sentential AI at a higher level but rather involves a different kind of representation or no representation at all, then it may or may not be possible to build an adequate natural language processor that uses solely the kind of sequential processing of current digital computers. It seems to me that it could turn out that how the computer actually works at the lowest level may be a relevant issue for natural language processing after all. As it stands, the usual kind of discussion that occurs about natural language processing in computers seems pretty much geared to a sentential AI interpretation. The usual goal is to process the natural language sentences into some sort of knowledge representation that is most easily interpreted as corresponding to an internal meaning representation or proposition in humans. The machines and programs used for the natural language processing simulations or programs are usually geared to sequential processing on traditional digital computers, so it is understandable why this should be so.

So that leaves syntactic analysis, semantic analysis, and pragmatics as the heart of most discussions of natural language processing. We tend to assume that a computer could be given the dictionary meanings of words, along with the type of speech for each and the rules of grammar, and by using this information process a natural language sentence. It should be a purely mechanical process of searching a database and classifying the sentence parts. But it is not at all this simple, mainly due to the problem of ambiguity at many levels. To interpret natural language sentences correctly, the system will not only have to parse the sentences grammatically but also associate the words with things in the world and with the general context of utterance of the sentence. While "parsing" is usually associated with syntactic analysis, parsing sentences correctly may even require clearing up any semantic and pragmatic issues that stand in the way of a correct syntactic analysis (for example, you may need to know context in order to decide whether "impact" is a noun or a verb; in recent years it has become increasingly used as a verb). And then once one has the syntactic analysis, semantics and pragmatics are needed to understand the full meaning of the sentence.

Thus there occurs an appreciation for the fact that understanding a natural language is much more complicated than just sorting out the words into parts of speech and looking up their definitions. Allen mentions the varied types of knowledge relevant to natural language understanding8:

Phonetic and phonological knowledge: how words are related to sounds.
Morphological knowledge: how words are built from more primitive morphemes (e. g., how "friendly" comes from "friend." Morphology deals with the different inflections of a word, the forms it can take: a noun can be singular or plural, a verb can have different tenses, and so forth. Morphemes include "run," "laugh," "non-," "-s," and "�es." Programs can be written to process tokens of words or even the more basic level of morphemes.
Syntactic knowledge: how sequences of words form correct sentences. Knowledge of the rules of grammar.
Semantic knowledge: how words have "meaning"; how words have reference (denotation) and associated concepts (connotations)
Pragmatic knowledge: "how sentences are used in different situations and how use affects the interpretation of the sentence," this involves the intentions and context of the conversation.
Discourse knowledge: how preceding sentences determine the meaning of a sentence, such as in the case of the referent of a pronoun.
World knowledge: general knowledge about for example, other users� beliefs and goals in a conversation.
Accommodating this breakdown of characteristics or types of knowledge within our earlier four subtopics of natural language processing, we would say syntactic analysis includes consideration of morphological and syntactic knowledge on the part of the natural language processor, semantic analysis includes consideration of semantic knowledge, and pragmatics includes consideration of pragmatic, discourse, and world knowledge. As I and some others use "pragmatics," it contains quite a lot, with one author listing "reference to the discourse context (including anaphora, inference of referents, and more extended relations of discourse coherence and narrative structure), conversational inference and implicature, and discourse planning and generation." I think Allen uses it in a narrower sense.

The result of a human person processing a sentence in a natural language is that the person understands the meaning of the sentence. We ignore consideration of whether a book, a play, or some other story or narrative has a single "meaning" intended by the author that is the meaning. Let's just say the sentence has a meaning that the processor understands. Also, we're not going to decide the issue between sentential AI and PDP/connectionist AI perspectives about the form of that meaning in humans, whether it is some sort of internal proposition or representation in the mind or brain of the processor. How it occurs in humans might be considered under the rubric of natural language understanding by investigators in artificial intelligence, philosophy, cognitive science, linguistics, computational linguistics, etc. Our immediate question instead is how we are to consider this topic for a computer.

I generally follow Allen's use of terms here, though many other authors have a similar understanding. As we attempt to model natural language processing, if we want to depict or represent the meaning of a sentence for such a model, we can't just use the sentence itself because ambiguities may be present. Words have multiple meanings, or senses. So, in the model, to represent the meaning of a sentence we need a more precise, unambiguous method of representation. We will use formal languages. Starting with a sentence in natural language, the result of syntactic analysis will yield a syntactic representation in a grammar; this is form is often displayed in a tree diagram or a particular way of writing it out as text. This type of syntactic representation might also be called a "structural description." Syntactic representations of language use context-free grammars, which show what phrases are parts of other phrases in what might be considered a context-free form. Then the result of the semantic analysis will yield the logical form of the sentence. Logical form is used to capture semantic meaning and depict this meaning independent of any such contexts. We then will proceed with a consideration of pragmatics, and so finally we need a general knowledge representation, which allows a contextual interpretation of the context-free form analysis and logical form. Keep in mind that I write as if the overall analysis proceeds in discrete stages, each stage yielding an output that serves as input for the next stage. One might view it this way logically, but some actual forms of natural language processing carry out several stages simultaneously rather than sequentially.

Here are some more terms. An interpretation process maps natural language sentences to the formal language, or from one formal language to others. But there are different types of interpretation process, depending on which formal language and stage is being considered. A parser is an interpretation process that maps natural language sentences to their syntactic structure or representation (result of syntactic analysis) and their logical form (result of semantic analysis). The parser uses the rules of grammar and word meanings (in a lexicon). This mapping could be sequential or simultaneous. A contextual interpretation maps the logical form to its final knowledge representation.

As already alluded to, there are different ways (separate or simultaneous) to accomplish the syntactic and semantic analysis, in short, the parsing, but there will be common elements in any such parsing. First there will be a grammar. The grammar specifies the legal ways for combining the units (syntactically and semantically) to result in other constituents. A lexicon indicating the types of speech for words will also be used; sometimes this is considered part of the grammar. Second, the processor will have an algorithm that, using the rules of the grammar, produces structural descriptions for a particular sentence. For example, the algorithm decides whether to examine the tokens from left to right or vice versa, whether to use a depth-first or breadth-first method, whether to proceed in a top-down or bottom-up method, etc. But it is possible that the algorithm will get into trouble if more than one rule applies, resulting in ambiguity, and thus the third component is an oracle, a mechanism for resolving such ambiguities. The type of ambiguity here could be lexical syntactic ambiguity (a word might be either a noun or verb, for instance), or structural syntactic ambiguity. This latter type of ambiguity involves the fact that there may be more than one way to combine the same lexical categories to result in a legal sentence.

To see how one might have conflicting syntactic analyses of a phrase, and thus need a way to decide them (an oracle), consider Steedman's example of structural syntactic ambiguity in the phrase "Put the book on the table." One way to analyze this is to consider it as a complete verb phrase, with the verb "put," the noun phrase "the book," and the prepositional phrase "on the table." Another way to analyze it is to consider "the book on the table" as itself the noun phrase, with no additional prepositional phrase. A sentence "Put the book on the table" most naturally invites the former interpretation, in other words, a direction to take the book and put it on the table. But yet if the phrase is included in the longer sentence "Put the book on the table in your pocket," then it invites the latter analysis, because it refers to a book already on the table. Depending on how the phrase it used in any particular sentence, grammar alone might not decide how to correctly analyze this phrase syntactically. (There is an additional ambiguity here about whether the book is already on the table or the table is in your pocket.)

